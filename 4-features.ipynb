{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability I: Feature Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright notice\n",
    "\n",
    "Parts of this code are adapted from https://pastebin.com/ETXc7Xma and the [Keras example](https://github.com/keras-team/keras/blob/master/examples/conv_filter_visualization.py), (c) 2015 - 2018, François Chollet, [MIT License](https://github.com/keras-team/keras/blob/master/LICENSE). This version (c) 2018 Fabian Offert, [MIT License](LICENSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Feature visualization has been an important area of research within machine learning in general and deep learning in particular at least since 2014 [Zeiler 2014, Simonyan 2014]. \"Deep Dream\", for instance, works by applying feature visualization techniques to images, albeit optimized for producing the kind of visuals it has become famous for. Since then, particularly with the invention of GANs, more elaborate methods have emerged that employ natural image priors to \"bias\" visualizations towards more \"legible\" images [Dosovitskiy 2016, Nguyen 2016a, Nguyen 206b, Nguyen 2017]. Recently, feature visualization and related methods have received a lot of attention as possible solutions to the problem of interpretability, most prominently in [Olah 2017, Olah 2018]. Nevertheless, almost all visualization methods rely on the principle of activation maximization. They visualize the learned features of a particular neuron/channel/layer by optimizing an input image to maximally activate this neuron/channel/layer. \n",
    "\n",
    "Below, we visualize the features of selected channels from the InceptionV1 (also known as GoogLeNet) network, trained on ImageNet. As [Olah 2018] point out, this particular network seems to produce much more legible visualizations then comparable (newer) networks, even without supplying natural image priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We are importing almost the same libraries as in the [\"Deep Dreaming\" notebook](3-deepdream.ipynb), except for two filter functions from SciPy, and two libraries to interface with the operating system. We need these later to run ImageMagick on our images to produce a nice montage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.ndimage.filters import gaussian_filter, median_filter\n",
    "from keras import backend as K\n",
    "from io import BytesIO\n",
    "import PIL.Image\n",
    "from IPython.display import clear_output, Image, display\n",
    "from subprocess import call\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "We use InceptionV1 as our model. As this architecture does not ship with Keras, we utilize [this custom implementation](https://github.com/fchollet/deep-learning-models/pull/59), with some minor changes/fixes. Most importantly, we change the softmax activation function into a linear activation function, as suggested in [Simonyan 2014]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are in the \"test\" phase, not the \"training\" phase, w.r.t. to the model we are analyzing\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "# Load Imodel with ImageNet pre-trained weights\n",
    "# Source: https://github.com/fchollet/deep-learning-models/pull/59\n",
    "import inception_v1_linear\n",
    "model = inception_v1_linear.InceptionV1(weights='imagenet', include_top=True) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define two sets of settings: one for visualizing the classes of the model (`settings_InceptionV1_classes`), and one for visualizing arbitrary layers of the model (`settings_InceptionV1_single`). Other than with V3, for InceptionV1 the input size is fixed to $224^2$ pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "settings_InceptionV1_classes = ['Predictions']\n",
    "settings_InceptionV1_single = ['Mixed_4b_Concatenated']\n",
    "settings = settings_InceptionV1_classes\n",
    "size = 224 # 224 for InceptionV1, variable for InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define which part of the layer we would like to visualize: just one particular channel (e.g. `filters = [8]` would look at the \"hen\" class in the `Predictions` layer), all available channels (`filters = None; sum_filters = False`), or the sum of all available channels (`filters = None; sum_filters = True`). If we are analyzing the output layer, the classes are defined according to [this list](https://github.com/happynear/caffe-windows/blob/master/examples/GoogLeNet/synset_words.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filters = list(range(1000)) # If None use all available filters (don't use None for prediction layers, define range!)\n",
    "sum_filters = False # If true, sum all filters, if false iterate over all filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing and deprocessing\n",
    "\n",
    "We are using the same image helper functions as in the [\"Deep Dreaming\" notebook](3-deepdream.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path)\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0) # Add extra dimension for mini batches (not used)\n",
    "    img = inception_v1_linear.preprocess_input(img) # 3D -> 1D\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Check ~/.keras/keras.json to make sure \"image_data_format\" is set to \"channels_last\"\n",
    "    # or print(K.image_data_format())\n",
    "    x = x.reshape((x.shape[1], x.shape[2], 3)) # \"Remove\" extra dimension, channels last\n",
    "    x /= 2.\n",
    "    x += 0.5\n",
    "    x *= 255.\n",
    "    x = np.clip(x, 0, 255).astype('uint8') # Clip to visible range\n",
    "    x = autotone(x)\n",
    "    return x\n",
    "\n",
    "# Simple resize function based on scipy\n",
    "def resize_img(img, size):\n",
    "    img = np.copy(img)\n",
    "    factors = (1, float(size[0]) / img.shape[1], float(size[1]) / img.shape[2], 1)\n",
    "    return scipy.ndimage.zoom(img, factors, order=1)\n",
    "\n",
    "# Simple save function based on scipy\n",
    "def save_image(img, fname):\n",
    "    pil_img = deprocess_image(np.copy(img))\n",
    "    scipy.misc.imsave(fname, pil_img)\n",
    "    \n",
    "def show_image(img, fmt='jpeg'):\n",
    "    img = deprocess_image(np.copy(img))\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(img).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More image preprocessing and deprocessing \n",
    "\n",
    "In addition, we define two functions to save images sequences. The first one (`save_image_numbered`) simly creates numbered sequences, the second one (`save_image_sweep`) includes a dictionary into the filename. We use this second function ofr hyperparameter sweeps (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def montage_images(folder, size):\n",
    "    print('Montaging...')\n",
    "    geometry = '-geometry \"' + str(size) + 'x' + str(size) + '+2+2>\" '\n",
    "    label = '-label \"%t\" '\n",
    "    output = folder + '/montage.jpg'\n",
    "    infiles = folder + '/*.jpg '\n",
    "    montage = 'montage  ' + label + infiles + geometry + output\n",
    "    call(montage, shell=True)\n",
    "    print('...done.')\n",
    "\n",
    "def save_image_numbered(img, nr, folder):\n",
    "    f = '{0:03d}'.format(nr)\n",
    "    p = folder + '/' + f + '.jpg'\n",
    "    save_image(img, p)\n",
    "    \n",
    "def save_image_sweep(img, filter, sweep, folder):\n",
    "    # Concatenate the list of values in the dictionary as strings\n",
    "    f = str(filter) + '_' + '_'.join(str(x) for x in list(sweep.values()))\n",
    "    p = folder + '/' + f + '.jpg'\n",
    "    print('Writing ' + p)\n",
    "    save_image(img, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization introduces priors into the loss function. By utilizing regularization, we end up with \"better\", more legible images. To start, we define a simple \"auto tone\" function (`autotone`) that normalizes each color channel in an image separately – exactly what Photoshop is doing in its \"auto tone\" function – to get more legible images. We also do not start with a plain gray image but with a gray image that includes some Gaussian white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize each color sepearately (Photoshop auto tone)\n",
    "def autotone(img):\n",
    "    img[:,:,0] = np.interp(img[:,:,0], [np.amin(img[:,:,0]), np.amax(img[:,:,0])], [0, 255])\n",
    "    img[:,:,1] = np.interp(img[:,:,1], [np.amin(img[:,:,1]), np.amax(img[:,:,1])], [0, 255])\n",
    "    img[:,:,2] = np.interp(img[:,:,2], [np.amin(img[:,:,2]), np.amax(img[:,:,2])], [0, 255])\n",
    "    return img\n",
    "\n",
    "def gray_square(size, variance): \n",
    "    img = np.random.normal(0, variance, (1, size, size, 3))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three other regularization functions are embedded into the gradient ascent function: a Gaussian blur function, and a median filter function. Their utilization is controlled by several hyperparameters that define how often these filters are applied (every four iterations, for instance) and how strong they are. By tuning these hyperparameters, either manually or automatically by means of hyperparameter sweeps (see below), we can find settings that subjectively produce better images, images that are more obviously representations of existing concepts. As so often, however, good is better then best, as the \"best\" images, i.e. the images that activate the layer/filter we are looking at the most, are usually just high-frequency noise. As pointed out by [Szegedy 2013], this link between adversarial examples and semantic is one of the most \"intruiging properties\" of neural networks that has [many interesting epistemological implications](https://arxiv.org/abs/1711.08042)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_ascent(x, \n",
    "                    iterations=1000, \n",
    "                    step=0.01, \n",
    "                    max_loss=None, \n",
    "                    blur_std=0, \n",
    "                    blur_every=8, \n",
    "                    median_fsize=5, \n",
    "                    median_every=4):\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        outs = fetch_loss_and_grads([x])\n",
    "        loss_value = outs[0]\n",
    "        grad_values = outs[1]\n",
    "        if max_loss is not None and loss_value > max_loss:\n",
    "            break\n",
    "        x += step * grad_values      \n",
    "        \n",
    "        if(i!=iterations-1): # No regularization on last iteration for good quality output \n",
    "            # Gaussian blur\n",
    "            if blur_std is not 0 and i % blur_every == 0 :\n",
    "                x = gaussian_filter(x, sigma=[0, blur_std, blur_std, 0])  \n",
    "            # Median filter\n",
    "            if median_fsize is not 0 and i % median_every == 0 :\n",
    "                x = median_filter(x, size=(1, median_fsize, median_fsize, 1))\n",
    "                \n",
    "    return x\n",
    "\n",
    "# Dictionary of the names of the layers and the layers themselves\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# Input of the first layer, in the example: model.input\n",
    "dream = model.layers[0].input \n",
    "\n",
    "# A TF variable (persistent)\n",
    "loss = K.variable(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Sweeping\n",
    "\n",
    "To find the \"right\" hyperparameters for the regularization techniques that we have introduced, we create a \"sweep\" function that conveniently applies sets of pre-defined parameters in all possible permutations to the same class/filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 sweeps generated\n"
     ]
    }
   ],
   "source": [
    "# Sweep over all possible permutations of allowed hyperparameters in a very un-pythonic but simple way\n",
    "\n",
    "\"\"\"\n",
    "# Sweep InceptionV1\n",
    "iterations = [2500]\n",
    "steps = [0.01, 0.1]\n",
    "max_losses = [None]\n",
    "blur_stds = [0]\n",
    "blur_everys = [2, 4, 8]\n",
    "median_fsizes = [3, 5, 7]\n",
    "median_everys = [2, 4, 8]\n",
    "variances = [0.01, 1]\n",
    "\"\"\"\n",
    "\n",
    "# Best values InceptionV1 from sweep above\n",
    "iterations = [1000]\n",
    "steps = [0.01]\n",
    "max_losses = [None]\n",
    "blur_stds = [0]\n",
    "blur_everys = [4]\n",
    "median_fsizes = [5]\n",
    "median_everys = [4]\n",
    "variances = [0.01]\n",
    "\n",
    "sweeps = []\n",
    "\n",
    "for iteration in iterations:\n",
    "    for step in steps:\n",
    "        for max_loss in max_losses:\n",
    "            for blur_std in blur_stds:\n",
    "                for blur_every in blur_everys:\n",
    "                    for median_fsize in median_fsizes:\n",
    "                        for median_every in median_everys:\n",
    "                            for variance in variances:\n",
    "                                sweeps.append({'iterations':iteration,\n",
    "                                                'step':step,\n",
    "                                                'max_loss':max_loss,\n",
    "                                                'blur_std':blur_std,\n",
    "                                                'blur_every':blur_every,\n",
    "                                                'median_fsize':median_fsize,\n",
    "                                                'median_every':median_every,\n",
    "                                                'variance':variance})\n",
    "                                \n",
    "print(str(len(sweeps)) + ' sweeps generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation maximiation\n",
    "\n",
    "We now run the activation maximization process for the selected layers/channels. The process is exactly the same as in the [\"Deep Dreaming\" notebook](3-deepdream.ipynb), except for a simpler loss function if we are looking at classes, as the last layer already gives us a single scalar loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterate only over the layers picked above and their available filters\n",
    "for layer_name in settings: \n",
    "    assert layer_name in layer_dict.keys(), 'Layer ' + layer_name + ' not found in model.' # Layer in model?\n",
    "    \n",
    "    # Create directory to hold frames\n",
    "    if not os.path.exists('4-features/' + layer_name):\n",
    "        os.makedirs('4-features/' + layer_name)\n",
    "    \n",
    "    x = layer_dict[layer_name].output # Output of the current layer\n",
    "        \n",
    "    if (filters == None and not sum_filters): channels = list(range(x.shape[3])) # Iterate over all filters\n",
    "    elif (filters == None and sum_filters): channels = [1] # Sum all filters\n",
    "    else: channels = filters\n",
    "    \n",
    "    # We might want to stop early and not be stuck with the same filters every time\n",
    "    np.random.shuffle(channels)\n",
    "    \n",
    "    for channel in channels:\n",
    "        \n",
    "        for sweep in sweeps:\n",
    "    \n",
    "            # We avoid border artifacts by only involving non-border pixels in the loss, offset by 2 on all sides\n",
    "            if (filters == None and sum_filters): loss = K.sum(K.mean(x[:, 2: -2, 2: -2, :]))\n",
    "            elif (filters == None and not sum_filters): loss = K.sum(K.mean(x[:, 2: -2, 2: -2, channel]))\n",
    "            # Classification layers just give a single probability, so no sum/mean/offset\n",
    "            else: loss = model.layers[-1].output[0, channel] # Always output of the last layer\n",
    "    \n",
    "            # Compute the gradients of the dream w.r.t. the loss.\n",
    "            grads = K.gradients(loss, dream)[0]\n",
    "            # Normalize gradients.\n",
    "            grads /= K.maximum(K.mean(K.abs(grads)), K.epsilon())\n",
    "\n",
    "            fetch_loss_and_grads = K.function([dream], [loss, grads])\n",
    "\n",
    "            img = gray_square(size, sweep['variance'])\n",
    "            img = gradient_ascent(img, \n",
    "                                  iterations=sweep['iterations'], \n",
    "                                  step=sweep['step'], \n",
    "                                  max_loss=sweep['max_loss'],\n",
    "                                  blur_std=sweep['blur_std'], \n",
    "                                  blur_every=sweep['blur_every'], \n",
    "                                  median_fsize=sweep['median_fsize'], \n",
    "                                  median_every=sweep['median_every'])\n",
    "            if (len(sweeps) > 1): save_image_sweep(img, channel, sweep, '4-features/' + layer_name)\n",
    "            else:\n",
    "                show_image(img)\n",
    "                save_image_numbered(img, channel, '4-features/' + layer_name)\n",
    "        \n",
    "    montage_images(layer_name, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 500 classes of InceptionV1 as per the hyperparameters defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](4-features/Predictions-500/montage.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some notable (hand-picked) classes\n",
    "Top to bottom: goldfish, Carassius auratus (2), loggerhead, loggerhead turtle, Caretta caretta (34), king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica (122), bee (310), bakery, bakeshop, bakehouse (416)\n",
    "\n",
    "\n",
    "![](4-features/Predictions-500/002.jpg)\n",
    "![](4-features/Predictions-500/034.jpg)  \n",
    "![](4-features/Predictions-500/122.jpg)\n",
    "![](4-features/Predictions-500/310.jpg)\n",
    "![](4-features/Predictions-500/416.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random 100 filters of InceptionV1 layer Mixed_4c_Concatenated as per the hyperparameters defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](4-features/4C-100R/montage.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random 100 filters of InceptionV1 layer Mixed_5b_Concatenated as per the hyperparameters defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](4-features/5B-100R/montage.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "- Dosovitskiy, Alexey, and Thomas Brox. \"Generating Images with Perceptual Similarity Metrics Based on Deep Networks.\" In Advances in Neural Information Processing Systems, 658–66, 2016. http://papers.nips.cc/paper/6157-generating- images-with-perceptual-similarity-metrics-based-on-deep-networks.\n",
    "- Nguyen, Anh, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. \"Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks.\" In Advances in Neural Information Processing Systems,\n",
    "3387–95, 2016. http://papers.nips.cc/paper/6519-synthesizing-the-preferred- inputs-for-neurons-in-neural-networks-via-deep-generator-networks.\n",
    "- Nguyen, Anh, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, and Jeff Clune. \"Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space.\" arXiv Preprint, 2017. https://arxiv.org/abs/1612.00005.\n",
    "- Nguyen, Anh, Jason Yosinski, and Jeff Clune. \"Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned by Each Neuron in Deep Neural Networks.\" arXiv Preprint arXiv:1602.03616, 2016. https://arxiv.org/abs/1602.03616.\n",
    "- Olah, Chris, Alexander Mordvintsev, and Ludwig Schubert. \"Feature Visualization.\" Distill, 2017. https://distill.pub/2017/feature-visualization.\n",
    "- Olah, Chris, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. \"The Building Blocks of Interpretability.\" Distill, 2018. https://distill.pub/2018/building-blocks/\n",
    "- Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. \"Deep Inside Convo- lutional Networks: Visualising Image Classification Models and Saliency Maps.\n",
    "- Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. \"Intriguing Properties of Neural Networks.\" arXiv Preprint arXiv:1312.6199, 2013. https://arxiv.org/abs/1312."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
