{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings/RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright notice\n",
    "\n",
    "Parts of this code are adapted from the [Keras example](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py), (c) 2015 - 2018, FranÃ§ois Chollet, [MIT License](https://github.com/keras-team/keras/blob/master/LICENSE). This version (c) 2018 Fabian Offert, [MIT License](LICENSE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We are using the Gensim and SpaCy NLP libraries that provide high-level interfaces for a lot of common NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import spacy\n",
    "import random\n",
    "from collections import *\n",
    "\n",
    "# python -m spacy download en\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, TimeDistributed, Flatten, Embedding\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained embeddings (Google News corpus, $10^{10}$ words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C binary format\n",
    "wv_news = gensim.models.KeyedVectors.load_word2vec_format('7-nlp/google300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computer_programmer', 0.5240642428398132),\n",
       " ('programer', 0.4957401156425476),\n",
       " ('engineer', 0.44241905212402344),\n",
       " ('mathematician', 0.41245338320732117),\n",
       " ('programmers', 0.4080742597579956),\n",
       " ('polymath', 0.4050711393356323),\n",
       " ('animator', 0.3985060453414917),\n",
       " ('Programmer', 0.3977847993373871),\n",
       " ('mechanical_engineer', 0.39661023020744324),\n",
       " ('Kaelin_Jacobson', 0.394615113735199)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_news.wv.most_similar(positive=['man', 'programmer'], negative=['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will generate a lot of output\n",
    "wv_news.wv.accuracy('7-nlp/questions-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-trained embeddings (\"In Search of Lost Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In Search of Lost Time\" is all about [links and similarities](https://en.wikipedia.org/wiki/Involuntary_memory): between times, places, things, senses, and people. Its arguably most famous scene is the \"Madeleine\" passage, where the experience of eating a simple [French coffee cake](https://en.wikipedia.org/wiki/Madeleine_(cake) leads the narrator to remember a childhood episode and, subsequently, his whole childhood and youth. How can we explore these links and similarities computationally? With word embeddings and RNNs, of course.\n",
    "\n",
    "> Many years had elapsed during which nothing of Combray, save what was comprised in the theatre and the drama of my going to bed there, had any existence for me, when one day in winter, as I came home, my mother, seeing that I was cold, offered me some tea, a thing I did not ordinarily take. I declined at first, and then, for no particular reason, changed my mind. She sent out for one of those short, plump little cakes called 'petites madeleines,' which look as though they had been moulded in the fluted scallop of a pilgrim's shell. And soon, mechanically, weary after a dull day with the prospect of a depressing morrow, I raised to my lips a spoonful of the tea in which I had soaked a morsel of the cake. No sooner had the warm liquid, and the crumbs with it, touched my palate than a shudder ran through my whole body, and I stopped, intent upon the extraordinary changes that were taking place. An exquisite pleasure had invaded my senses, but individual, detached, with no suggestion of its origin. And at once the vicissitudes of life had become indifferent to me, its disasters innocuous, its brevity illusory - this new sensation having had on me the effect which love has of filling me with a precious essence; or rather this essence was not in me, it was myself. I had ceased now to feel mediocre, accidental, mortal. Whence could it have come to me, this all-powerful joy? I was conscious that it was connected with the taste of tea and cake, but that it infinitely transcended those savours, could not, indeed, be of the same nature as theirs. Whence did it come? What did it signify? How could I seize upon and define it?\n",
    "I drink a second mouthful, in which I find nothing more than in the first, a third, which gives me rather less than the second. It is time to stop; the potion is losing its magic. It is plain that the object of my quest, the truth, lies not in the cup but in myself. The tea has called up in me, but does not itself understand, and can only repeat indefinitely with a gradual loss of strength, the same testimony; which I, too, cannot interpret, though I hope at least to be able to call upon the tea for it again and to find it there presently, intact and at my disposal, for my final enlightenment. I put down my cup and examine my own mind. It is for it to discover the truth. But how? What an abyss of uncertainty whenever the mind feels that some part of it has strayed beyond its own borders; when it, the seeker, is at once the dark region through which it must go seeking, where all its equipment will avail it nothing. Seek? More than that: create. It is face to face with something which does not so far exist, to which it alone can give reality and substance, which it alone can bring into the light of day.\n",
    "And I begin again to ask myself what it could have been, this unremembered state which brought with it no logical proof of its existence, but only the sense that it was a happy, that it was a real state in whose presence other states of consciousness melted and vanished. I decide to attempt to make it reappear. I retrace my thoughts to the moment at which I drank the first spoonful of tea. I find again the same state, illumined by no fresh light. I compel my mind to make one further effort, to follow and recapture once again the fleeting sensation. And that nothing may interrupt it in its course I shut out every obstacle, every extraneous idea, I stop my ears and inhibit all attention to the sounds which come from the next room. And then, feeling that my mind is growing fatigued without having any success to report, I compel it for a change to enjoy that distraction which I have just denied it, to think of other things, to rest and refresh itself before the supreme attempt. And then for the second time I clear an empty space in front of it. I place in position before my mind's eye the still recent taste of that first mouthful, and I feel something start within me, something that leaves its resting-place and attempts to rise, something that has been embedded like an anchor at a great depth; I do not know yet what it is, but I can feel it mounting slowly; I can measure the resistance, I can hear the echo of great spaces traversed.\n",
    "Undoubtedly what is thus palpitating in the depths of my being must be the image, the visual memory which, being linked to that taste, has tried to follow it into my conscious mind. But its struggles are too far off, too much confused; scarcely can I perceive the colourless reflection in which are blended the uncapturable whirling medley of radiant hues, and I cannot distinguish its form, cannot invite it, as the one possible interpreter, to translate to me the evidence of its contemporary, its inseparable paramour, the taste of cake soaked in tea; cannot ask it to inform me what special circumstance is in question, of what period in my past life.\n",
    "Will it ultimately reach the clear surface of my consciousness, this memory, this old, dead moment which the magnetism of an identical moment has travelled so far to importune, to disturb, to raise up out of the very depths of my being? I cannot tell. Now that I feel nothing, it has stopped, has perhaps gone down again into its darkness, from which who can say whether it will ever rise? Ten times over I must essay the task, must lean down over the abyss. And each time the natural laziness which deters us from every difficult enterprise, every work of importance, has urged me to leave the thing alone, to drink my tea and to think merely of the worries of to-day and of my hopes for to-morrow, which let themselves be pondered over without effort or distress of mind.\n",
    "And suddenly the memory returns. The taste was that of the little crumb of madeleine which on Sunday mornings at Combray (because on those mornings I did not go out before church-time), when I went to say good day to her in her bedroom, my aunt Leonie used to give me, dipping it first in her own cup of real or of lime-flower tea. The sight of the little madeleine had recalled nothing to my mind before I tasted it; perhaps because I had so often seen such things in the interval, without tasting them, on the trays in pastry-cooks' windows, that their image had dissociated itself from those Combray days to take its place among others more recent; perhaps because of those memories, so long abandoned and put out of mind, nothing now survived, everything was scattered; the forms of things, including that of the little scallop-shell of pastry, so richly sensual under its severe, religious folds, were either obliterated or had been so long dormant as to have lost the power of expansion which would have allowed them to resume their place in my consciousness. But when from a long-distant past nothing subsists, after the people are dead, after the things are broken and scattered, still, alone, more fragile, but with more vitality, more unsubstantial, more persistent, more faithful, the smell and taste of things remain poised a long time, like souls, ready to remind us, waiting and hoping for their moment, amid the ruins of all the rest; and bear unfaltering, in the tiny and almost impalpable drop of their essence, the vast structure of recollection.\n",
    "And once I had recognized the taste of the crumb of madeleine soaked in her decoction of lime-flowers which my aunt used to give me (although I did not yet know and must long postpone the discovery of why this memory made me so happy) immediately the old grey house upon the street, where her room was, rose up like the scenery of a theatre to attach itself to the little pavilion, opening on to the garden, which had been built out behind it for my parents (the isolated panel which until that moment had been all that I could see); and with the house the town, from morning to night and in all weathers, the Square where I was sent before luncheon, the streets along which I used to run errands, the country roads we took when it was fine. And just as the Japanese amuse themselves by filling a porcelain bowl with water and steeping in it little crumbs of paper which until then are without character or form, but, the moment they become wet, stretch themselves and bend, take on colour and distinctive shape, become flowers or houses or people, permanent and recognisable, so in that moment all the flowers in our garden and in M. Swann's park, and the water-lilies on the Vivonne and the good folk of the village and their little dwellings and the parish church and the whole of Combray and of its surroundings, taking their proper shapes and growing solid, sprang into being, town and gardens alike, from my cup of tea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yield_file(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    " \n",
    "    # By default, these yields nice, clean lists of sentence words\n",
    "    def __iter__(self):\n",
    "        # File only has line brakes at paragraph boundaries\n",
    "        # Always remove possible BOMs with vim -c \"set nobomb\" -c wq! myfile\n",
    "        for paragraph in open(self.filename):\n",
    "            for sentence in paragraph.split('.'):\n",
    "                \n",
    "                # Use only lower case\n",
    "                sentence = sentence.lower()\n",
    "\n",
    "                # Remove all punctuation\n",
    "                exclude = set(string.punctuation)\n",
    "                sentence = ''.join(char for char in sentence if char not in exclude)\n",
    "\n",
    "                # Remove whitespaces\n",
    "                sentence = sentence.strip()\n",
    "\n",
    "                # Line as list\n",
    "                sentence = sentence.split()\n",
    "                \n",
    "                # Only return non-empty lines\n",
    "                if len(sentence) > 0: yield sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.840269923210144),\n",
       " ('laundress', 0.8244017958641052),\n",
       " ('historian', 0.82276451587677),\n",
       " ('patronage', 0.8200148344039917),\n",
       " ('jardin', 0.8188639879226685),\n",
       " ('south', 0.808866560459137),\n",
       " ('balloon', 0.8062515258789062),\n",
       " ('manager', 0.8060844540596008),\n",
       " ('painter', 0.803223729133606),\n",
       " ('ambassador', 0.799126148223877)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = yield_file('7-nlp/proust_ascii.txt') \n",
    "wv_proust = gensim.models.Word2Vec(sentences, size=300, window=5, min_count=5, workers=4)\n",
    "wv_proust.wv.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "# wv_proust.wv.accuracy('7-nlp/questions-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Embeddings with named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yield_file_tagged(object):\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    \n",
    "    def _tag_word(self, word):\n",
    "        text = word.text\n",
    "        if word.ent_type_: tag = word.ent_type_\n",
    "        else: tag = word.pos_\n",
    "        return text + '|' + tag\n",
    " \n",
    "    # By default, these yields nice, clean lists of sentence words\n",
    "    def __iter__(self):\n",
    "        # File only has line brakes at paragraph boundaries\n",
    "        # Always remove possible BOMs with vim -c \"set nobomb\" -c wq! myfile\n",
    "        for paragraph in open(self.filename):\n",
    "            # SpaCy magic\n",
    "            doc = nlp(paragraph)\n",
    "    \n",
    "            # Detect and merge entitites\n",
    "            for ent in doc.ents:\n",
    "                ent.merge(tag=ent.root.tag_, lemma=ent.text, ent_type=ent.root.ent_type_)\n",
    "    \n",
    "            # Detect and merge noun chunks\n",
    "            for nc in doc.noun_chunks:\n",
    "                while len(nc) > 1 and nc[0].dep_ not in ('advmod', 'amod', 'compound'):\n",
    "                    nc = nc[1:]\n",
    "                nc.merge(tag=nc.root.tag_, lemma=nc.text, ent_type=nc.root.ent_type_)\n",
    "            \n",
    "            for sentence in doc.sents:\n",
    "                words = []\n",
    "                for word in sentence:\n",
    "                    if not word.is_space: \n",
    "                        words.append(self._tag_word(word))\n",
    "                            \n",
    "                yield words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = yield_file_tagged('7-nlp/proust_ascii.txt') \n",
    "wv_proust_tagged = gensim.models.Word2Vec(sentences, size=300, window=5, min_count=5, workers=4)\n",
    "wv_proust_tagged.save('7-nlp/wv_proust_tagged.gensimmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=10617, size=300, alpha=0.025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Morel|PERSON', 0.8458129167556763),\n",
       " ('Bloch|PERSON', 0.8372248411178589),\n",
       " ('Saint-Loup|ORG', 0.8185210824012756),\n",
       " ('Swann|PERSON', 0.8098665475845337),\n",
       " ('M. de Norpois|ORG', 0.8037635087966919),\n",
       " ('M. de Guermantes|PERSON', 0.7948862910270691),\n",
       " ('Elstir|PERSON', 0.7704919576644897),\n",
       " ('Robert|PERSON', 0.765677809715271),\n",
       " ('Odette|PROPN', 0.7484169006347656),\n",
       " ('Cottard|PERSON', 0.7473478317260742)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_proust_tagged_reloaded = gensim.models.Word2Vec.load('7-nlp/wv_proust_tagged.gensimmodel')\n",
    "print(wv_proust_tagged_reloaded)\n",
    "wv_proust_tagged_reloaded.wv.most_similar(positive=['Albertine|PERSON', 'M. de Charlus|PERSON'], negative=['I|PRON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Proust from scratch with a character-level LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 1000000 # Limit dataset to DATA_SIZE characters\n",
    "SEQUENCE_LEN = 40 # Order of the language model\n",
    "SEQUENCE_STEP = 3 # Redundancy of the training samples\n",
    "EPOCHS = 100\n",
    "LSTM_SIZE = 128\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (characters): 1000000\n",
      "Unique characters: 44\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '2', '5', '7', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Sequences: 333320\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Read the whole corpus into memory, only use lower case\n",
    "text = ''\n",
    "with open('7-nlp/proust_ascii.txt') as f: text = f.read().lower()\n",
    "\n",
    "# If the dataset is too large, the loss either explodes (this model), or converges way too fast (other models)\n",
    "# Conretely, the loss will slightly rise during later epochs, and if the epoch \"takes too long\" will explode\n",
    "# \"Cutting\" epochs \"dips\" the loss, so that, averaged over epochs, it still decreases\n",
    "# For this model, ~1M characters seem too work reasonably well\n",
    "text = text[:DATA_SIZE]\n",
    "    \n",
    "print('Corpus length (characters):', len(text))\n",
    "\n",
    "# Create the set of all characters that appear in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "print(chars)\n",
    "\n",
    "# Create two dictionaries to \"translate\" from a char to an index and vice versa\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Cut the text in semi-redundant sequences of SEQUENCE_LEN characters\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - SEQUENCE_LEN, SEQUENCE_STEP):\n",
    "    # A sequence of length SEQUENCE_LEN\n",
    "    sentences.append(text[i: i + SEQUENCE_LEN])\n",
    "    # The following character\n",
    "    next_chars.append(text[i + SEQUENCE_LEN])\n",
    "print('Sequences:', len(sentences))\n",
    "\n",
    "# Generate one-hot vectors, x contains the sequence of SEQUENCE_LEN size, y contains the next char\n",
    "print('Vectorization...')\n",
    "\n",
    "x = np.zeros((len(sentences), SEQUENCE_LEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building model...')\n",
    "\n",
    "char_model = Sequential()\n",
    "char_model.add(LSTM(LSTM_SIZE, input_shape=(SEQUENCE_LEN, len(chars))))\n",
    "char_model.add(Dense(len(chars)))\n",
    "char_model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=LR)\n",
    "char_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print('Model built\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get an actual character from the probability array returned as a prediction by the model,\n",
    "# we need to employ some probability math\n",
    "def sample(preds, diversity):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / diversity\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, diversities, length):\n",
    "     \n",
    "    # A random point in the text\n",
    "    start_index = random.randint(0, len(text) - SEQUENCE_LEN - 1)\n",
    "    \n",
    "    # An empty dictionary\n",
    "    returns = {}\n",
    "    \n",
    "    for diversity in diversities:\n",
    "        \n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + SEQUENCE_LEN]\n",
    "        generated += sentence\n",
    "\n",
    "        for i in range(length):\n",
    "            \n",
    "            # Vectorize sentence\n",
    "            x_pred = np.zeros((1, SEQUENCE_LEN, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            # Predict next character probabilities\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            \n",
    "            # Add char\n",
    "            next_index = sample(preds, diversity)  \n",
    "            next_char = indices_char[next_index]\n",
    "            generated += next_char\n",
    "            \n",
    "            # \"Move\" the sentence one char over to predict the next next character\n",
    "            sentence = sentence[1:] + next_char\n",
    "        \n",
    "        returns[diversity] = generated\n",
    "          \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A callback function to sample some text after each epoch\n",
    "def on_epoch_end(epoch, logs):\n",
    "    print('\\n' + '### DIVERSITY: 0.5 ###')\n",
    "    print(generate(char_model, [0.5], 300)[0.5] + '\\n')\n",
    "\n",
    "history = char_model.fit(x, \n",
    "           y, \n",
    "           batch_size=BATCH_SIZE, \n",
    "           epochs=EPOCHS, \n",
    "           callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)],\n",
    "           validation_split=0.1)\n",
    "char_model.save('7-nlp/char_model_100.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DIVERSITY: 0.1 ###\n",
      "more, to pay an infinitely scrupulous at once again, and the presencial before he had not to leave her that he was not the princess of the pink of the present its withing the place of odette's lever, who had supposed that she was not the same said of the probuil in a serious changess who had been able to see her all the least of the probull of the pink of the place in the same said of the portralte talls face of the probull of the country was to him by the family that he was not the first time there was nothing for the princess that he had been able to see her all the last that he was not the seepers and conceation.\n",
      "of and to see the course of the place in the same said of the promise of the presence of the present its sure do really the same time a disting of the probull of the provision of the presence of the street chimpselse, which she had not to the same of the pleasure which the most seck the company of the pleasure which she had not allowed that it was a moment, although in the same said, i can love the same scretrati\n",
      "\n",
      "\n",
      "### DIVERSITY: 0.2 ###\n",
      "rfeo which was being rendered on the flusted more than an incitimity of the flowers which he had not to be surtain, that i was not to see her that he was not the same tands of making him as the must's suprections, and which had begun to reme be seen to her to him a long fire which had been so shew a words of the first time, and the place when i was to be such an uncertainte of the place in the same said of a care from her that my aunt spoke to him from the men to her that my aunt spoken of the presence of the same said of the present in the street chimasion near the presence of the propersy of the probull of the same send which he had not to be success of the country was all that it was the family which he had seen me that my mother, and then it was a moment, and then we would see her fance of the same said of a street taste, and to see her had the same disportanter whom it was not the early desire to have that she was not the same said of which he had seen me that mme. verdurin was not the presencion of the possible sit sur\n",
      "\n",
      "\n",
      "### DIVERSITY: 0.3 ###\n",
      "ffer some interest, might be a source of odette had gone out a falously in the state of the place of a lave the sundays seemed to see himself for the same said of a calle, who could not have been in a time to the arestally places on the portrains which he was accompledged to spend the sun and different us the same screer tamble of an impression of another companion of the presence of the street couns of the same feeling which she was a beal own precious weather who can love her the street clissed whom he said the counte of an expression, but in the same recognised that i was no longer more than the seatifical and a sunday, or that i had not to be seen for the resert to the man who had not to disting the same surbract from of the pink mamber, that she was not the tepred care in the state of the place in which she would have come to see them by the thought of the street couns of the pink, which had been so stranged a part of the girst, and the sense, which she had not taken in the same saigh of the caress of her time with his \n",
      "\n",
      "\n",
      "### DIVERSITY: 0.5 ###\n",
      "come, rattling all over the place, and then there was no idea is suddenly wented to be at the taste of the appearance of the pleasures which there was nothing but of the streets of combray of the seating from the pripe of certain usent and that mme. sa\"moureilall ie her family which he found should he had succeeded the same sclats, and the seafity of an old from the sun it not the piano streem, and the particular companalle before this serious changes in any almost incalled house, but my father and brokem to a fat even caught out for the pinner the whole who had best that she was not allowed that it was in the little phrase of chuman scruting this whom he was not an iestill characters that there has the sersonation of the house of the part of the thought of her hearts, ded he would only happenings what on her eyes who had been the same to spoke all over the family could reade the house of the familiar part of the dutyor of the presence of its association in sleep whom it is not, to be able to take end man of some one of them\n",
      "\n",
      "\n",
      "### DIVERSITY: 1.0 ###\n",
      "nts which shewed, against a dark background from her are, after dropy at air suppresd face the overyous of occlour's bauln. order for supeling rose packly and endine. she dopers when, again illusinested brilk an ansiesc ef that.\" caught be wight.\n",
      "nomed swanns wall, upon her hall they remelved empeciely leve. but i would reply had since a friends of musical her or swann's cheadby and the spiding eyes - or for a secret of the cyubture. had ceased to sudden, and the thought of her thoughts so steple that it must have been else, come up in poict to rosung than i colluced an ampleter; peasants duting the torterned would in the gadessys in the walking to encouring it an immance of the ways, and only by indepensences me aloud whom he was new had interruption, that he middless had ready about might not be pictured app to seri goding, and hours now body he to pleaded who was in love. in the house, he was to meet laugh offering acquaided thears the paremten, daughan by what this cased, all that closed an oin of which, bergotter for th\n",
      "\n",
      "\n",
      "### DIVERSITY: 2.0 ###\n",
      "e's eyes, with my aunt.\n",
      "during that autude jouds acciditt; exclaiable knise accefter!\" the joustively, of dligm. wrom wait mere ditecately it. thos. , learthen? affrewed that, or gaddera, \"frinart-'blor dhewn stre? you.\"\n",
      "; i'vid,we'sy suca veepeaum,' well flor\n",
      "noyot , any of a  gllate never, meloze hust beli pirkly in!\"o, crolara,ce'r, if catmed us. imong evtif-blte.\" for het thytr\"y, ceecmsered me a plow which dr, who gardong. butweed, from wabturn tk ut of existenca! muchtrriowhis twat\" illusincly an, occurt, evenidi'tches? whe ve bontogering\"lys, she have yroouskquee. no(, cotsianyx-grohing alou; youpiowbacq's fitht and grallaa\n",
      "thing to difiningurald.\n",
      "weren frammer.o,h wo lomen phrader! bued, on fbyat, in swartlanaidd; radiinved,t would a-vavitusity, sgothfulely, m. legear-nuve birc herhalvary.nded pheases,\"w\"yon sounlong (two fronnatt.\n",
      "thatld, rather  panersions, duth pahinad jr;attious;s o'fle-tricaness, baspe;  juy with inantfocsable, was \"she sharppoused, bounaflels. ugowen.\"\n",
      "the firthfusly mo-d: yate, she hal, swann \n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('7-nlp/char_model_100.hdf5')\n",
    "# Try different diversity values, i.e. less and more \"exotic\" predictions\n",
    "for d in [0.1, 0.2, 0.3, 0.5, 1.0, 2.0]:\n",
    "    print('\\n' + '### DIVERSITY: ' + str(d) + ' ###')\n",
    "    print(generate(loaded_model, [d], 1000)[d] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Proust from scratch with an unsmoothed maximum likelihood character level language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_char_lm(fname, order=4):\n",
    "    data = open(fname).read()\n",
    "    lm = defaultdict(Counter)\n",
    "    for i in range(len(data)-order):\n",
    "        history, char = data[i:i+order], data[i+order]\n",
    "        lm[history][char]+=1\n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        return [(c, cnt/s) for c, cnt in counter.items()]\n",
    "    outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "    return outlm\n",
    "\n",
    "def generate_letter(lm, history, order):\n",
    "        history = history[-order:]\n",
    "        dist = lm[history]\n",
    "        keys = [k[0] for k in lm[history]]\n",
    "        values = [k[1] for k in lm[history]]\n",
    "        return np.random.choice(keys, p=values)\n",
    "        \n",
    "def generate_text(lm, order, nletters=1000):\n",
    "    # Seed is a random pick from vocabulary\n",
    "    history = random.choice(list(lm.keys()))\n",
    "    out = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history, order)\n",
    "        history = history[-order:] + c\n",
    "        out.append(c)\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ORDER: 1 ###\n",
      "pldise Ely waghir at, ther myhomenscincast ndit rcrin dind pathe orer l wer all om mmby ysed h whitusisal, at w o he, ha ile aby th alsh ssiffache dntey ws meve stterentat Vermiche ithads tesideran ifasith tongor co t Le orofare onf hather, ad wabe pstosstolanthise at t d mathero I anxal thare or ce thawhern aulome og por mpre amen Conenay ime, tofe l s, we arnghou, linle ice tmsincthasosonso alich I tha thet tof t ea d tre, a e lich friexhel fouberes a teblderlad sp ang Intcuther how trero d mam her. myene sm, arfr spll thes, rrarme lf, whinttin iosa. d wht higr, epeweminowa int warthent ghem ucarney puno t me msarevemus s f. th, hes hedrta tr outloke ite oun ot hurrrechees, e jula werink bl e on o tomysthato bing h lur, tatendialathes th l Bulle raltharatos showhena w y me ldof y foof Anco fe icerin asir fusisound henknt finad ing ie s n culyoullllff bud keduthe bean prof ughowerhin, tese wertesott Bly f, wedon d ldr theape fr ie, bs l d thef se d hinsicecedoulenof tenor h Mmy to wo \n",
      "\n",
      "### ORDER: 2 ###\n",
      " did bety hip's. Sant ders, he sativer's ge wined fid the brarkabold sh I som, elfixtion, ing hadonce frolatencoulde the my my tand samparl mor her whick ut con to thave a doichad press itylerthe was the Gue ockend th who, wentito tep my le va's tiould even whou me. \"Thing berind noting mings to hol, alise obseplidescring he so,\" entid ber that tay petakee it, all Jus' of to love is liked? Yout inlit, dentilen moth, ing aserds flont, elventrat had (some me counoyalif ted ong,\" \"Nor, was pary.\" \"Ind have Duccaugh to merebor ressurgeshe com hat andicand ascuoure ther the fal ametreme Chappy, whint the Chad not yourists wilbet thating unt rearien, for such, olons ped. Nisen it, to dichad a cuponstace. \"the a prow ons abow of in him of whad se me whadesse hich me; she But hommore to anxismenswer he as whour pieven Albe stin meneved us supied to All, the of M. I ber wast, whichouries I sherld the whey 'etraid asturis evoul mem, a cout tas shemple of to sul. \"It in he int dist I cer. \"But re\n",
      "\n",
      "### ORDER: 3 ###\n",
      " book lace wates scarrast ame one so had bey a me the left Sain makes what eyes,\" and which every the effectly unchoise mome - which conjoymented carry forture you, by a like Cambray we as the Vill it her of himself the so keption yound emore, rement, roup, the quaine's vaguermaid: \"He's her regar of that lettine had be she done,\" same ans who, shed be one had fond to openetime whold going to-day thoribled this have to bend the was which as her, which have tell herengage of dom that I had, than imaginnersonal comte of for as: \"Eithout, unched from and I, when andown incapturned ally woman were.\n",
      "My at to dist's gone which in their highly evenius you much ove, in Mme. As someonieceive returesaw ther was in the 'Gilbecomposite a conce why des upon the passure the waitiny of were atage objectureless; sounderable is ever, an a mong ther ever excise of sorts unsult makes at in studesiant might - alling for a give.\n",
      "If ally the have to her, he has to done made am in read. And ther onces the fo\n",
      "\n",
      "### ORDER: 4 ###\n",
      "Suffeur,'- yourself, whether by the gree though his only nor the the prudently towards memories to be, those scattern his personata - seeing it,\" said to repla well, painstead one of out of my before seclusive.\n",
      "At the return to a polite that intell you, M. de Charlus life? As a little, even it was not such must lie flag, still notion which serve merely to bready to remarks of a confidents to false forgotte's: \"I did idea of the prompted to regular school. I too, in tries. But fleeting room and that Mme. She find These state the name; I decided away, the has became othere now was a few her the Words with a corrow, which we withought had durin's how consting. And ones which she was poet her you minds the tain such an exquise's not converself my feeling the have lite asylum making for friend. It is those chose to me interes, who was were only old my long effortist, thing as who were popularly intended his of the this contral past my greaten.\"\n",
      "In a gigologne, which the most moving to my mi\n",
      "\n",
      "### ORDER: 5 ###\n",
      " Before we should because to my mind that their tiny flowers in the would never four.\"\n",
      "Her was complement some to combines of the dissociation by personal what of which one or the idea caring to a preliminate drive, applicate profit if the little grounded with a public, with his eye to see the name, but the people and and I don't like an one of my really to Saint-Loup with seemed them appearance, but more a letter these world is office forms of the house. \"But you were together - one by Mlle. Vinteuil is same pottery, but, which we are in discontinues which she had had aristocratic, what he same that talking\" as prospects on him.\n",
      "\"Oh, I've gone from the stupid, as artist smile: \"But, do us altered. \"We having hand impossibility in her. As for though you play-actress was not be said to be call the gave them amusing her? Obviously puerile it had been two would waiting on his play at all think, the opport with my grandmother side, exception had seems, but Francoise had assumed articular r\n",
      "\n",
      "### ORDER: 10 ###\n",
      "n various race, doubtless she has a thousand plans, fall in love, the charming man who gave her five francs the pound; to see them after dinner he would say: \"It must be at the start, I might (but never forget my invitation, the true existence of the slightest, I can see that it was quite narrow, this intellectual might survive a secondary truth. For even if I had not seen them and her husband's esteem whereas Rachel who had just given Mme. Verdurin alone and has need of these scenes so precious brow. My grandmother was right when they were the ground floor apartment with joy, and where Jupien in tears in my company without any abstract ideas) she had contrived to talk with you, I hope. You understandings that I wished to attract, while determined that surrounds them appeared to me by the Princess was in the wilds like that?\" \"Oh, I forgot for an instant, do you hear that you want the movements like gardens whose flowering innocents.\" Everybody knew anything which one of those great an\n",
      "\n",
      "### ORDER: 15 ###\n",
      " He belonged also to the fine name Palamede. The truth was that Mme. de Villeparisis; I did not stop to consider this visit which I paid to the Combray district, which was perhaps the person who was a Major in every regiment in her country, who had been waiting for me before I knew them. But alas, those I now so much longed for, I could find no more. The years which had transformed me to his own likeness; compared with which the great renegade of Protestantism platonised in the German manner for a Germany prehistorically sentimental and aulic, ringing all the changes of temperature (against the danger and - I should add, perhaps - the by no means delightful, in fact it was he himself who was looking at the clock to see how much he was changing. He resembled an old book of the middle classes would have thrilled me by giving me back the blue purity of the atmosphere, stifling as a breathless summer evening, the resounding sky growls like a tawny lion, and everyone in the room with an ast\n",
      "\n",
      "### ORDER: 20 ###\n",
      "mple trick, can beguile, without taking the lift or being seen from the main staircase, a smaller private staircase, no longer in use, offered me its steps so skilfully arranged, one close above another, that there seemed to exist in their gradation a perfect proportion of the same kind as those which, in colours, scents, savours, often arouse in us a peculiar, sensuous pleasure. But the pleasure to be derived from mixing only with the people of one's own world, and reminding oneself that, one's own being the best of all possible worlds, the ill-informed contempt of 'outsiders' may be disregarded. Perhaps she felt that - were she to arrive incognito at the Grand Hotel, he had complained to the manager there (my friend) that the royal standard of Luxembourg was not flown in front of the hotel the purr of the engine, delighted when she learned these tidings, and to believe that it was no longer there, for when I was sure she would return as on the day when Saint-Loup had left me! In a sh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for order in [1,2,3,4,5,10,15,20]:\n",
    "    print('### ORDER: %d ###' % order)\n",
    "    lm = train_char_lm('7-nlp/proust_ascii.txt', order)\n",
    "    print(generate_text(lm, order) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ORDER: 15 ###\n",
      " panes are mere transparencies. He would try to make out Odette. And then, how were we not to include in it the uncles who are not really related by blood, being the uncles only of their nephews' wives. The Messieurs de Charlus are indeed so convinced that a certain man in society who had taken her out of the room that it remembered, which he had been bored to tears in her friend's presence, as much as the dazzling glories of the beach, albeit my jealousy was suddenly revealing her anxiety to look my best before Albertine had paid me. I saw her in successive years of my life occupying, with regard to whom one completely forgot about dinner and the time; here again as at Balbec I had not been mistaken, for he would think of me now no longer had any idea what it is?\" \"Well . . . I've heard people say 'the Rohans' or in contempt, as she herself still supposed that one could examine here in public, for, the Princesse de Parme and announced by one verbal blow after another to sprinkle with their conjunction, their stopping together for time enough just to pay my devotions to the view which for once it revealed over the hill against which I had prepared beforehand, a refusal when, a Province having demanded its independence of action with which I was filled by the murmurs of the Grandee of Spain business nowadays, though he had been thinking of all the nondescript, resinous, dull, indigestible, and fruity smell of the sea, they come and breathe the salt air through the post, notwithstanding her tale of forty winters fully told, wiping herself with staying in her room.\n",
      "\"Is Mme. Blandais makes me when she stares at those people like that of shares in a company, rises with the demand and falls when it is offered in the market.\n",
      "Everything that related to Balbec was like one of those zoological gardens with their gorgeous colouring and incomprehensible that the gentleman who has earned them,\" Albertine repeated, alluding to certain purely imaginary. It must on no account, when she had company, what I said to her: \"Aow to you too, Matame la Marquise de Cambremer!\" exclaimed Cottard, \"she'll be telling us presently that she was dead. For nothing is ever lost; a missing jewel turns up again; when the quantity of fish, pullets, grouse, woodcock, pigeons, brought in dressed and garnished and piping hot by breathless waiters who slid over the polished oak of the door and the ordering of our dinner (he laid great stress on our choosing 'butcher's meat,' the fowls being presumably nothing to say against your nervous fear of food, nor of fresh air, and lots of clever men. In any case, if the very modest advancement of Jupien's niece had been sincere in saying that I think she has a poor opinion of Charlus and came here chiefly to please me, that my visit was so great a happiness for her without payment for at least five years, swollen no less by bottles of Portugal and Eau des Souverains, irons, razors, and strops, than by the fact that, while he was asleep, the intention to travel would reawaken in him (without his remembering that I had entered the room to pay Mme. Swann one of them?\"\n",
      "\"Good lord, no! That is to say, to love her still and to forget the land that they were laying up for themselves, not one minute, not one pleasure, the allurement to work that they were going to the Verdurins, to take Albertine upon whose comradeship she must so far have reckoned that, in the course of my ride come upon a mythological subjects (I had seen photographs of her are always blurred. I did not rightly know how Gilberte's features remained compressed in this letter reference was made to various other little places the King of Bavaria, and finally of Prince Von, to whom it was addressed as a rule to all these other people. If people are different strata which are not alike (there were indeed several heiresses in view, but after all the oaths that she swore to me that nothing of the sort! She does not care for polite speeches has grown up round it, as round the supper-table of M. and Mme. Helvetius, the greatest ladies in Paris and that she had had a slight stroke.\n",
      "We made our way back along the floor towards me; I could not take any part in the quarrel, but he warned them so that they might easily be found wanting in none of them, loving them all, and I ask myself whether there was any question of the colour of which would then have known that feeling which has at all a novel sound. Personally, I deplore this. How often had his credit with a duchess, built up of the yearly accumulation of those eddies which, in the women, there emerged a raucous voice, recalled the fabulous monster to the proportion 'thirty per cent' which M. de Charlus and Morel. Often they had already passed out of sight of land. No doubt the tenant, before entering the courtyard, in the street might do her?\"\n",
      "My uncle advised Swann not to see Odette. You know what is meant by taste. By Jove, it is Morel that ought really to come there in the morning. The ways? More than that: create. It is face to face with his hostess. Thus a young painter, brought up by her, and married her cousin.\" (My grandmother, Mme. de Mortemart need not have been seemly. But when her footman began to come into contact with the forgotten outlines of Albertines, separate from that earlier one because I am unmoved by a certain type of humour, that I am often struck by my wife's wit. For you will find him in bed with another person in whom I had been unable to endure the defects of Bloch, the chief magistrate did not take them very seriously. As for the Cambremers. He was one of those model bedrooms which you seem to suppose, she is the last person in the whole course of the conversation of the heat of the day, at a later date. One of them, however, was not at all surprised if she got in after the Elevation of the Cross.\n",
      "So long as I had not yet asked anything of Albertine: \"Oh yes, she always loved any interesting people in her house. If Forcheville and the Verdurins; paused for a moment with myself. Moreover, what he had seen and what he had supposed: \"But don't let us go any farther,\" he would invariably says: \"The old Duchess of So-and-so,\") would prescribe almost automatic laughter of childhood, a spasmodic discharge which, in those days to the death of the pleasure of life. \"Oh, I don't know, something like, 'Suffer me, Sir,' (at the very most, she whom we have long been lying in a berth on board one of those fashionable houses in which, at one time, the memory of the Queen.\"\n",
      "Meanwhile my father took counsel with the faithful, the consecrated ground of the hours that Mme. de Guermantes; I had quite forgotten my grandmother and clung with my lips to her face as I drew level with her knee, a sort of broad-brimmed garden hat, nothing more - the cause of endless suffering, the higher the coefficient rose. For instance at the Princesse de Parme, listen to tragedy or opera, but from the social point of view, a universal process of causation which sooner or later been admitted into the angle of eyebrow and cheek his own monocle, the sole instrument that comes to our lips, whether I evoked them by mimicry and association of ideas suddenly brought back remorseful memories and I was overwhelmed with that profound peace of which I had then remarked in the delivery and gestures of Aricie, Ismene and Hippolyte. It was not too far away to hear her. \"No she cannot possibly be a substitute for the bulk of it what was art still, to introduce, like a breath of air, something of the same kind of pleasure the Princess, I did not look any more for it under 'S' than under 'C'; she did indeed give little afternoon party in order to settle certain little things, in everything and you were nearing your grave, it is because we ourselves must die.\n",
      "Many years had elapsed), and entertained the Rothschilds in importance, became an agreeable incident. Sometimes it made me taste a pleasure that we ourself make outweigh - even if we consider how great a proportion of light and shade upon the morning of our departure, to see me look so unhappy, \"I don't mind telling you,\" an expression that they would be made under cover, I should not love Gilberte, but that, notwithstanding this, having failed to enter the hotel by the hall, that is to say - it is tremendously smart! How nice you look with it! Every inch a gentleman. All you want now is a title!\" she concluded that the model was beautiful was beyond dispute, but not to the Princesse de Parme barely greeted me, she turned back to the person in question. Besides, was I so certain that the bath-woman and the vegetable-woman, awnings were spread to protect them from the world. He had hardly had time to go very far. Now that motor-cars have come in, it would be prudent to have him recalled and the Director of Political Affairs would receive what she had so that it should be her and none but her shadow, the pure and vivid colouring would otherwise remain uncultivated mentally, rough in their friends' wives, tried to make herself late. She neither could not be applied to the Princesse de Parme. But who cares about Mme de Varambon now? My friend here knew about all this, but as you once by chance entered the temple of impurity, I have nothing to fear, she does everything that is more or less her way of expressing humour, good nature and stupid mistakes.\" Swann was too subtle not to perceive that the Duchess, instead of the Dreyfus case has made things more serious. Swann ought to have told him the truth.\n",
      "Physically, she was passing through a village, we thought we could make out. It is not the only thing to which, playing upon the title of the still open book which would have amused you. It is in the volume which covers his mission to Mme. Bontemps was afraid that you might be utilised afterwards to imitate. Whereas there were several Guermantes in a dreamy tone. \"I know the little d'Ambresacs, do you? Dear me, you have some startling revelations one of these meals, which may give an\n"
     ]
    }
   ],
   "source": [
    "order = 15\n",
    "print('### ORDER: %d ###' % order)\n",
    "lm = train_char_lm('7-nlp/proust_ascii.txt', order)\n",
    "print(generate_text(lm, order, nletters=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN resources\n",
    "\n",
    "Research areas: text-to-speech, translation, image captions, handwriting, music, ...\n",
    "\n",
    "- [Keras recurrent layers](https://keras.io/layers/recurrent/)\n",
    "- [Emergence of grid-like representations by training recurrent neural networks to perform spatial localization](https://openreview.net/pdf?id=B17JTOe0-)\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [The unreasonable effectiveness of Character-level Language Models](http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139)\n",
    "- [Sasha Poflepp: Recursion (sic!)](http://pohflepp.net/Recursion)\n",
    "- [Polyphonic Music Generation Using Tied Parallel Networks](https://www.cs.hmc.edu/~ddjohnson/tied-parallel/)\n",
    "- [A Connectionist Approach to Algorithmic Composition](http://www.indiana.edu/~abcwest/pmwiki/pdf/todd.compmusic.1989.pdf)\n",
    "- [Four Experiments in Handwriting with a Neural Network](https://distill.pub/2016/handwriting/)\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [do androids dream of cooking?](https://gist.github.com/nylki/1efbaa36635956d35bcc)\n",
    "- [Folk music style modelling using LSTMs](https://github.com/IraKorshunova/folk-rnn)\n",
    "- [RNN Bible Bot](https://twitter.com/RNN_Bible)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
