{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Limits of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "One core problem of contemporary machine learning techniques, as pointed out by [Marcus 2017], [Lake et. al. 2017], and many others, is a lack of symbolic reasoning skills in purely statistically trained (\"deep learning\" type) models like convolutional neural networks. While [Lake et. al. 2017] propose many possible technological fixes for this problem (for instance, the integration of a game-engine style physics model into a network), we could ask with [Agre 1995] if these fixes do not simply defer an inherently philosophical problem: the problem that, despite neural networks being general function approximators, intelligent generalization can not be modeled with general function approximation. As [Marcus 2017] writes: \n",
    "\n",
    "> The real problem lies in misunderstanding what deep learning is, and is not, good for. The technique excels at solving closed-end classification problems, in which a wide range of potential signals must be mapped onto a limited number of categories, given that there is enough data available and the test set closely resembles the training set.\n",
    "But deviations from these assumptions can cause problems; deep learning is just a statistical technique, and all statistical techniques suffer from deviation from their assumptions.\n",
    "\n",
    "In other words, \"true\" generalization would be closer to Cartesian compositionality then to a very large mapping betwen domains. Again [Marcus 2017]:\n",
    "\n",
    "> [S]ome problems cannot, given real- world limitations, be thought of as classification problems at all. Open-ended natural language understanding, for example, should not be thought of as a classifier mapping between a large finite set of sentences and large, finite set of sentences, but rather a mapping between a potentially infinite range of input sentences and an equally vast array of meanings, many never previously encountered. In a problem like that, deep learning becomes a square peg slammed into a round hole, a crude approximation when there must be a solution elsewhere.\n",
    "\n",
    "As an illustration of his argument, [Marcus 2017] proposes a toy example: generalizing from even to odd numbers:\n",
    "\n",
    "> Distilling the broad-ranging problems of language down to a simple example that I believe still has resonance now, I ran a series of experiments in which I trained three- layer perceptrons (fully connected in today’s technical parlance, with no convolution) on the identity function, $f(x) = x$, e.g, $f(12)=12$.\n",
    "\n",
    "The technical setup for the experiment is simple:\n",
    "\n",
    "> 1997-vintage networks were, to be sure, simpler than current models — they used no more than three layers (inputs nodes connected to hidden nodes connected to outputs node), and lacked Lecun’s powerful convolution technique. But they were driven by backpropagation just as today’s systems are, and just as beholden to their training data.\n",
    "\n",
    "The results are straightforward:\n",
    "\n",
    "> Every time I ran the experiment, using a wide variety of parameters, the results were the same: the network would (unless it got stuck in local minimum) correctly apply the identity function to the even numbers that it had seen before (say 2, 4, 8 and 12), and to some other even numbers (say 6 and 14) but fail on all the odds numbers, yielding, for example f(15) = 14. [...] Odd numbers were outside the training space, and the networks could not generalize identity outside that space. Adding more hidden units didn’t help, and nor did adding more hidden layers. Simple multilayer perceptrons simply couldn’t generalize outside their training space [...].\n",
    "\n",
    "In the following, we will reproduce the experiment described in [Marcus:2017]. The intention behind this is not so much the validation of their results but an excercise in critical technical practice ([Agre 1997]), in translating between philosophical and technical contexts. Additionally, given the availability of tools compared to 1997, we can implement some simple variations of the proposed experiment and potentially show that the results still hold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "To be able to use mechanisms like differentiation and backpropagation we need a high-level library that abstracts these mathematical details away from us. We are using Keras, the de-facto standard for high-level prototyping for machine learning. Keras is a front-end to the Tensorflow framework, which is one of the ost widely used machine learning frameworks (for a comparison, see machine learning frameworks). Because we are operating in high-dimensional vectir space (and because Keras/Tensorflow use its datatypes), we are also using Numpy, the Python library for scientific computing. Finally, we are importing a single helper function from Scipy to split our data into train and test sets, and some helper functions to plot our activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import expit as sigmoid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "These constants are the hyperparameters for our neural network. Exactly as proposed by [Marcus 2017], we are implementing the identity function $f(x) = x$ for binary integers. Our parameters are:\n",
    "\n",
    "- Our integers have `BITS` bits, so our network should learn to apply the identify function for $\\frac{2^{16}}{2}$ possible *even* integers.\n",
    "- After training we will test our network on `VAL` validation samples that are kept back.\n",
    "- Our hidden layers consist of `HIDDEN` units each.\n",
    "- We use `LAYERS` layers total.\n",
    "- We train the network for `EPOCHS` \"epochs\". One epoch is a complete \"run\" of all available training samples, i.e. a single forward and backward pass with all available samples.\n",
    "- We pass the samples to the network in batches of `BATCHES` each, this saves memory.\n",
    "- We do integrate some dropout layers to prevent overfitting (particularly important in this case).\n",
    "- Finally we \"mix in\" `MIX` odd numbers into our even numbers to find the threshold that enables the network to generalize for even and odd numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BITS = 16\n",
    "VAL = 25\n",
    "HIDDEN = 128\n",
    "LAYERS = 3\n",
    "EPOCHS = 100\n",
    "BATCHES = 128\n",
    "DROPOUT = False\n",
    "MIX = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to approximate\n",
    "\n",
    "We define the most simple function possible, the identity function $f(x) = x$. To allow some additional experiments, we also define the bitwise-not function, $f(x)=\\sim{x}$. Note that the numpy implementation of bitwise not returns an array of truth values. To convert this into an array of 0s and 1s, we simply multiply it by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \n",
    "    # Identity\n",
    "    return x\n",
    "    \n",
    "    # Bitwise not\n",
    "    # return  np.logical_not(x) * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training data\n",
    "\n",
    "Our training data is simple: a set of binary values, encoded as `BITS` dimensional vectors (e.g. `[0, 0, 1, 1, 1]`), provides both the input and desired output of our network. To produce this set, we create two two-dimensional arrays (`x_even` and `x_odd`) that will hold the even and odd numbers, respectively. We then fill the arrays from the bottom up, by iterating over the range of all $2^{\\text{BITS}}$ possible numbers with a step width of 2. In each step, we write the current number to the array that holds the even numbers, and the current number minus one to the array that holds the odd numbers. We randomly shuffle both arrays, and reserve a part for testing, and another (smaller) part for validation. Finally, we apply the function defined above to a copy of the array of even numbers, and use this as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 bit numbers = 65536 possible values.\n",
      "Train set: 29468 even binary vectors (0 odd mixed into total set).\n",
      "Test set: 3275 even binary vectors (0 odd mixed into total set).\n",
      "Validation set: 25 even binary vectors and 25 odd binary vectors.\n"
     ]
    }
   ],
   "source": [
    "MAX = 2**BITS # BITS bits = 2^BITS values\n",
    "\n",
    "# Make space for all even BITS-bit numbers and all odd BITS-bit numbers\n",
    "x_even = np.zeros((int(MAX/2), BITS), dtype=int)\n",
    "x_odd = np.zeros((int(MAX/2), BITS), dtype=int)\n",
    "\n",
    "# Fill from the bottom, converting to binary on the fly\n",
    "for i in range(0, MAX, 2):\n",
    "    x_even[int(i/2),:] = np.array(list(np.binary_repr(i, BITS)))\n",
    "    x_odd[int(i/2),:] = np.array(list(np.binary_repr(i+1, BITS)))\n",
    "\n",
    "# Randomly shuffle everything    \n",
    "np.random.shuffle(x_even)\n",
    "np.random.shuffle(x_odd)\n",
    "\n",
    "x_val_even = x_even[-VAL:,:] # Save last VAL random even numbers for prediction \n",
    "x_even = x_even[:-VAL,:] # (use the rest for training and testing)\n",
    "x_val_odd = x_odd[-VAL:,:] # Save last VAL random odd numbers for prediction (throw the rest away)\n",
    "\n",
    "x_even[:MIX,:] = x_odd[:MIX,:]\n",
    "# Randomly shuffle again   \n",
    "np.random.shuffle(x_even)\n",
    "\n",
    "# Apply function we would like to model\n",
    "y = f(x_even)\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_even, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print(str(BITS) + ' bit numbers = ' + str(MAX) + ' possible values.')\n",
    "print(\"Train set: \" + str(len(x_train)) + ' even binary vectors (' + str(MIX) + ' odd mixed into total set).')\n",
    "print(\"Test set: \" + str(len(x_test)) + ' even binary vectors (' + str(MIX) + ' odd mixed into total set).')\n",
    "print(\"Validation set: \" + str(len(x_val_even)) + ' even binary vectors and ' + str(len(x_val_odd)) + ' odd binary vectors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Keras conveniently provides us with abstractions for many of the most commonly used building blocks of neural networks. For this experiment, we are using six different components: fully connected layers, ReLu activation, a sigmoid activation, a binary cross-entropy loss function, and backporpagation by means of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected layer\n",
    "\n",
    "This is the standard, multilayer perceptron layer, where every unit of a layer is connected to every unit of the layer before and after. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLu activation function\n",
    "\n",
    "[Rectified linear units](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) are the most popular neural network units at the time of writing. Despite their \"linear look\", the [activation function](https://en.wikipedia.org/wiki/Activation_function) used: \n",
    "\n",
    "$$f(x)=\\max(0,x)$$\n",
    "\n",
    "is actually nonlinear ([piecewise-linear](https://en.wikipedia.org/wiki/Piecewise_linear_function), to be precise) *and* differentiable. Why nonlinear? Well, a linear function has to satisfy the condition \n",
    "\n",
    "$$\\forall_{x,y}: f(x) + f(y) = f(x+y)$$\n",
    "\n",
    "For ReLu, \n",
    "\n",
    "$$f(-1) = -1$$\n",
    "$$f(1) = 1$$\n",
    "$$f(0) = 0$$ \n",
    "\n",
    "Its derivative is then simply \n",
    "\n",
    "$$f'(x)={\\begin{cases}0&{\\text{for }}x<0\\\\1&{\\text{for }}x\\geq 0\\end{cases}}$$\n",
    "\n",
    "Interestingly, the \"almost-linearity\" of ReLus does not impede their universality in approximating functions (see [these examples](https://www.quora.com/Is-a-single-layered-ReLu-network-still-a-universal-approximator/answer/Conner-Davis-2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f75702f3048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([max(0, x) for x in np.arange(-10.0,10.0,0.1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid activation function\n",
    "\n",
    "The [sigmoid function](https://en.wikipedia.org/wiki/Logistic_function), also called logistic function, looks roughly like the simple step function but has non-zero gradients everywhere, which makes it fully differentiable. It is defined as\n",
    "\n",
    "$$f(x)={\\frac {1}{1+e^{-x}}}$$\n",
    "\n",
    "with the derivative\n",
    "\n",
    "$$f'(x)=f(x)(1-f(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f75790bda20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([sigmoid(x) for x in np.arange(-10.0,10.0,0.1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary cross-entopy loss function\n",
    "\n",
    "Because our input and output are binary numbers, we are dealing with a [multi-label classification]( https://en.wikipedia.org/wiki/Multi-label_classification) problem. The loss function best suited to address this scenario is binary [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy). In mathematical terms, minimizing this function equals minimizing the [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), here the divergence of the probability distribution inherent in the training set and the probability distribution created by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation via stochastic gradient descent\n",
    "\n",
    "First, a quick derivative reminder:\n",
    "\n",
    "$$\\frac{df}{dx} = \\lim_{x \\to +\\infty} \\frac{f(x+h)-f(x)}{h}$$\n",
    "\n",
    "[Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) is based on the observation that if the multi-variable function $f(x)$ is defined and differentiable in a neighborhood of a point $a$, then $f(x)$ decreases fastest if one goes from $a$ in the direction of the negative gradient of $f$ at $a$, $f(a) -\\nabla f(a)$. The algorithm for a system with the loss function $Loss$ would then be\n",
    "\n",
    "$$w_n \\leftarrow w_n - \\underbrace{\\eta}_\\text{step size} \\nabla_{w_{n}} Loss(w_n)$$\n",
    "\n",
    "for every training sample, where $\\nabla_{w_{n}}$ ([Del](https://en.wikipedia.org/wiki/Del)) denotes the partial derivative w. r. t. the weight $w_n$, which can usually be derived by means of the [chain rule](https://en.wikipedia.org/wiki/Chain_rule): \n",
    "\n",
    "$$(f\\circ g)'=(f'\\circ g)\\cdot g'$$\n",
    "\n",
    "[Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) adds an additional element to the process to increase throughput: batch size. Instead of supplying one training sample per iteration (where iteration defines one forward and backward pass), multiple (random) samples are concatenated and the network is trained on these concatenated \"mini-batches\". Essentially, we are trading dimensionality for speed which is intuitively a good choice if we use GPUs for training.\n",
    "\n",
    "Example: $f(x)=x^4−3x^3+2$, with derivative $f'(x)=4x3−9x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The local minimum occurs at 2.2499646074278457\n"
     ]
    }
   ],
   "source": [
    "# From calculation, it is expected that the local minimum occurs at x=9/4\n",
    "\n",
    "x = 6 # The algorithm starts at x=6\n",
    "eta = 0.01 # Step size multiplier\n",
    "precision = 0.00001\n",
    "previous_step_size = 1/precision # Some large value\n",
    "\n",
    "df = lambda x: 4 * x**3 - 9 * x**2\n",
    "\n",
    "while previous_step_size > precision:\n",
    "    prev_x = x\n",
    "    x += -eta * df(prev_x)\n",
    "    previous_step_size = abs(x - prev_x)\n",
    "\n",
    "print(\"The local minimum occurs at \" + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 128)               2176      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 37,264\n",
      "Trainable params: 37,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Dense(HIDDEN) is a fully-connected layer with HIDDEN hidden units.\n",
    "# In the first layer, you must specify the expected input data shape, here BITS-dimensional vectors.\n",
    "model.add(layers.Dense(HIDDEN, activation='relu', input_dim=BITS))\n",
    "for _ in range(LAYERS-1):\n",
    "    model.add(layers.Dense(HIDDEN, activation='relu'))\n",
    "    if (DROPOUT): model.add(layers.Dropout(0.5))\n",
    "# Sigmoid activation for multilabel classification\n",
    "# https://github.com/keras-team/keras/issues/741\n",
    "# https://en.wikipedia.org/wiki/Multi-label_classification\n",
    "model.add(layers.Dense(BITS, activation='sigmoid'))\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# Binary crossentropy for multilabel classification\n",
    "# https://github.com/keras-team/keras/issues/741\n",
    "# https://en.wikipedia.org/wiki/Multi-label_classification\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29468/29468 [==============================] - 1s 17us/step - loss: 0.6400 - acc: 0.6730\n",
      "Epoch 2/100\n",
      "29468/29468 [==============================] - 0s 14us/step - loss: 0.5009 - acc: 0.8781\n",
      "Epoch 3/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 0.2921 - acc: 0.9504\n",
      "Epoch 4/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.1247 - acc: 0.9933\n",
      "Epoch 5/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0493 - acc: 0.9998\n",
      "Epoch 6/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0248 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0156 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0111 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "29468/29468 [==============================] - 0s 13us/step - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "29468/29468 [==============================] - 0s 13us/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 9.7507e-04 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 9.2127e-04 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 8.7203e-04 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 8.2719e-04 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 7.8599e-04 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 7.4807e-04 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 7.1307e-04 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 6.8072e-04 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 6.5072e-04 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 6.2291e-04 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 5.9705e-04 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 5.7288e-04 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 5.5036e-04 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 5.2905e-04 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 5.0948e-04 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 4.9084e-04 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 4.7340e-04 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 4.5695e-04 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 4.4145e-04 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 4.2680e-04 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "29468/29468 [==============================] - 0s 13us/step - loss: 4.1301e-04 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 3.9988e-04 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 3.8750e-04 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 3.7573e-04 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 3.6458e-04 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "29468/29468 [==============================] - 0s 11us/step - loss: 3.5399e-04 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "29468/29468 [==============================] - 0s 14us/step - loss: 3.4387e-04 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 3.3426e-04 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 3.2509e-04 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 3.1636e-04 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 3.0801e-04 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 3.0004e-04 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.9241e-04 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.8511e-04 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "29468/29468 [==============================] - 0s 13us/step - loss: 2.7810e-04 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "29468/29468 [==============================] - 0s 13us/step - loss: 2.7141e-04 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "29468/29468 [==============================] - 0s 15us/step - loss: 2.6499e-04 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.5881e-04 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.5288e-04 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.4716e-04 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.4170e-04 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.3642e-04 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.3135e-04 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.2647e-04 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.2177e-04 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.1722e-04 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.1283e-04 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.0860e-04 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.0451e-04 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 2.0057e-04 - acc: 1.0000\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.9676e-04 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.9307e-04 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.8951e-04 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "29468/29468 [==============================] - 0s 14us/step - loss: 1.8604e-04 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "29468/29468 [==============================] - 0s 13us/step - loss: 1.8271e-04 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.7946e-04 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.7632e-04 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.7326e-04 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.7030e-04 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "29468/29468 [==============================] - 0s 14us/step - loss: 1.6744e-04 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "29468/29468 [==============================] - 0s 14us/step - loss: 1.6466e-04 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.6196e-04 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.5933e-04 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.5679e-04 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.5431e-04 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.5190e-04 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "29468/29468 [==============================] - 0s 13us/step - loss: 1.4955e-04 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "29468/29468 [==============================] - 0s 14us/step - loss: 1.4728e-04 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.4505e-04 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.4289e-04 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.4079e-04 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "29468/29468 [==============================] - 0s 12us/step - loss: 1.3874e-04 - acc: 1.0000\n",
      "3275/3275 [==============================] - 0s 18us/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCHES)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "We are now using the samples that we kept back to evaulate how well our model has learned to approximate the identity function. First, we define some \"pretty printing\" options, then we let our model make predictions and compare the evaluation samples with the outputs of the prediction, digit by digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVEN\n",
      "\n",
      "\u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 61452\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 44972\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m 41062\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 43160\n",
      "\u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 7888\n",
      "\u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m 60178\n",
      "\u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m 8778\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 35216\n",
      "\u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m 5390\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 45300\n",
      "\u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m 28258\n",
      "\u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 15020\n",
      "\u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 28824\n",
      "\u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m 23066\n",
      "\u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m 590\n",
      "\u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 11784\n",
      "\u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 7104\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m 43702\n",
      "\u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 63304\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 48336\n",
      "\u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m 24466\n",
      "\u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 58768\n",
      "\u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m 51794\n",
      "\u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m 4196\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m 34782\n",
      "\n",
      "ODD\n",
      "\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[91m0\u001b[0m 35140\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[91m0\u001b[0m 33866\n",
      "\u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[91m0\u001b[0m 62928\n",
      "\u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[91m0\u001b[0m 19684\n",
      "\u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[91m0\u001b[0m 5204\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[91m0\u001b[0m 34764\n",
      "\u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[91m0\u001b[0m 7148\n",
      "\u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[91m0\u001b[0m 58638\n",
      "\u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[91m0\u001b[0m 55526\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[91m0\u001b[0m 38506\n",
      "\u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[91m0\u001b[0m 19626\n",
      "\u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[91m0\u001b[0m 62352\n",
      "\u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[91m0\u001b[0m 10252\n",
      "\u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[91m0\u001b[0m 25248\n",
      "\u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[91m0\u001b[0m 55438\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[91m0\u001b[0m 44986\n",
      "\u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[91m0\u001b[0m 50470\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[91m0\u001b[0m 33360\n",
      "\u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[91m0\u001b[0m 53498\n",
      "\u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[91m0\u001b[0m 31226\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[91m0\u001b[0m 34784\n",
      "\u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[91m0\u001b[0m 5280\n",
      "\u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[91m0\u001b[0m 16782\n",
      "\u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[91m0\u001b[0m 40866\n",
      "\u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[92m1\u001b[0m \u001b[92m0\u001b[0m \u001b[92m1\u001b[0m \u001b[91m0\u001b[0m 59290\n"
     ]
    }
   ],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n",
    "\n",
    "x_pred_even = np.rint(model.predict(x_val_even)).astype(np.uint8)\n",
    "x_pred_odd = np.rint(model.predict(x_val_odd)).astype(np.uint8)\n",
    "\n",
    "print('EVEN\\n')\n",
    "for y in range(VAL):\n",
    "    for x in range(BITS):\n",
    "        if f(x_val_even)[y, x] == x_pred_even[y, x]:\n",
    "            print(colors.ok + str(x_pred_even[y, x]) + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + str(x_pred_even[y, x]) + colors.close, end=' ')\n",
    "    # https://stackoverflow.com/questions/41069825/convert-binary-01-numpy-to-integer-or-binary-string#41069967\n",
    "    print(x_pred_even[y,:].dot(2**np.arange(x_pred_even[y,:].size)[::-1]))\n",
    "\n",
    "print('\\nODD\\n')\n",
    "for y in range(VAL):\n",
    "    for x in range(BITS):\n",
    "        if f(x_val_odd)[y, x] == x_pred_odd[y, x]:\n",
    "            print(colors.ok + str(x_pred_odd[y, x]) + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + str(x_pred_odd[y, x]) + colors.close, end=' ')\n",
    "    # https://stackoverflow.com/questions/41069825/convert-binary-01-numpy-to-integer-or-binary-string#41069967\n",
    "    print(x_pred_odd[y,:].dot(2**np.arange(x_pred_odd[y,:].size)[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "- Agre, Philip E. *Computation and Human Experience*. Cambridge University Press, 1997.\n",
    "- Agre, Philip E. \"The Soul Gained and Lost. Artificial Intelligence as a Philosophical Project.\" Stanford Humanities Review 4, no. 2 (1995): 1–19.\n",
    "- Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. \"Multilayer Feedforward Networks Are Universal Approximators.\" Neural Networks 2, no. 5 (1989): 359–66.\n",
    "- Lake, Brenden M., Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. \"Building Machines That Learn and Think Like People.\" Behavioral and Brain Sciences 40 (2017).\n",
    "- Marcus, Gary. \"Deep Learning: A Critical Appraisal.\" arXiv Preprint arXiv:1801.00631, 2018. https://arxiv.org/abs/1801.00631v1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
